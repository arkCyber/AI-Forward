#!/usr/bin/env python3
"""
==============================================================================
NGINX AI Gateway Service Test Script
==============================================================================
Description: å…¨é¢æµ‹è¯• NGINX AI ç½‘å…³æœåŠ¡ï¼ŒåŒ…æ‹¬æµå¼å“åº”å’Œå¤–éƒ¨è®¿é—®
Author: Assistant
Created: 2024-12-19
Version: 1.0

This script tests:
- æµå¼å“åº”ï¼ˆStream Responseï¼‰åŠŸèƒ½
- å¤–éƒ¨ç”¨æˆ·è®¿é—®èƒ½åŠ›
- ä¸åŒ AI æœåŠ¡ç«¯ç‚¹
- è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
- é€Ÿç‡é™åˆ¶åŠŸèƒ½
- å¹¶å‘è¯·æ±‚å¤„ç†
==============================================================================
"""

import asyncio
import json
import sys
import time
import aiohttp
import requests
from datetime import datetime
from typing import Dict, List, Optional, AsyncGenerator
from concurrent.futures import ThreadPoolExecutor
import threading


class NGINXAIServiceTester:
    """NGINX AI ç½‘å…³æœåŠ¡æµ‹è¯•å™¨"""
    
    def __init__(self, base_url: str = "http://localhost"):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            base_url: NGINX ç½‘å…³åŸºç¡€ URL
        """
        self.base_url = base_url.rstrip('/')
        self.session = None
        self.test_results = {}
        
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—ä¿¡æ¯"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        color_map = {
            "INFO": "\033[0;36m",    # Cyan
            "SUCCESS": "\033[0;32m", # Green
            "WARNING": "\033[1;33m", # Yellow
            "ERROR": "\033[0;31m",   # Red
            "STREAM": "\033[0;35m"   # Magenta
        }
        reset = "\033[0m"
        color = color_map.get(level, "")
        print(f"{color}[{timestamp}] [{level}] {message}{reset}")
        
    def test_nginx_health(self) -> bool:
        """æµ‹è¯• NGINX ç½‘å…³å¥åº·çŠ¶æ€"""
        self.log("ğŸ” Testing NGINX Gateway Health...")
        
        try:
            response = requests.get(f"{self.base_url}/nginx-health", timeout=5)
            if response.status_code == 200:
                self.log("âœ… NGINX Gateway is healthy", "SUCCESS")
                return True
            else:
                self.log(f"âŒ NGINX Gateway health check failed: {response.status_code}", "ERROR")
                return False
        except Exception as e:
            self.log(f"âŒ NGINX Gateway connection failed: {e}", "ERROR")
            return False
    
    def test_service_health(self) -> Dict[str, bool]:
        """æµ‹è¯•å„ä¸ªæœåŠ¡å¥åº·çŠ¶æ€"""
        self.log("ğŸ” Testing Backend Services Health...")
        
        services = {
            'ai-router': f"{self.base_url}/health",
            'stats': f"{self.base_url}/stats"
        }
        
        results = {}
        for service_name, url in services.items():
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    self.log(f"âœ… {service_name} is healthy", "SUCCESS")
                    results[service_name] = True
                else:
                    self.log(f"âš ï¸  {service_name} returned status {response.status_code}", "WARNING")
                    results[service_name] = False
            except Exception as e:
                self.log(f"âŒ {service_name} health check failed: {e}", "ERROR")
                results[service_name] = False
        
        return results
    
    def test_non_streaming_chat(self, service: str = "api", model: str = "gpt-3.5-turbo") -> bool:
        """æµ‹è¯•éæµå¼èŠå¤©å®Œæˆ"""
        self.log(f"ğŸ¤– Testing Non-Streaming Chat ({service})...")
        
        endpoint_map = {
            'api': f"{self.base_url}/api/v1/chat/completions",
            'v1': f"{self.base_url}/v1/chat/completions", 
            'deepseek': f"{self.base_url}/deepseek/v1/chat/completions",
            'lingyiwanwu': f"{self.base_url}/lingyiwanwu/v1/chat/completions",
            'ollama': f"{self.base_url}/ollama/v1/chat/completions",
            'forward': f"{self.base_url}/forward/v1/chat/completions"
        }
        
        url = endpoint_map.get(service, endpoint_map['api'])
        
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system", 
                    "content": "You are a helpful AI assistant. Please respond concisely."
                },
                {
                    "role": "user", 
                    "content": f"Hello! This is a test message from NGINX gateway via {service} endpoint. Please respond with 'Test successful via {service}' and current time."
                }
            ],
            "stream": False,
            "max_tokens": 100,
            "temperature": 0.7
        }
        
        headers = {
            "Content-Type": "application/json",
            "User-Agent": "NGINX-AI-Gateway-Test/1.0"
        }
        
        try:
            start_time = time.time()
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                self.log(f"âœ… Non-streaming chat successful ({service})", "SUCCESS")
                self.log(f"   Response: {content[:100]}...", "SUCCESS")
                self.log(f"   Response time: {response_time:.2f}s", "SUCCESS")
                return True
            else:
                self.log(f"âŒ Non-streaming chat failed ({service}): {response.status_code}", "ERROR")
                self.log(f"   Response: {response.text[:200]}", "ERROR")
                return False
                
        except Exception as e:
            self.log(f"âŒ Non-streaming chat error ({service}): {e}", "ERROR")
            return False
    
    async def test_streaming_chat(self, service: str = "api", model: str = "gpt-3.5-turbo") -> bool:
        """æµ‹è¯•æµå¼èŠå¤©å®Œæˆ - å…³é”®åŠŸèƒ½æµ‹è¯•"""
        self.log(f"ğŸŒŠ Testing Streaming Chat ({service})...", "STREAM")
        
        endpoint_map = {
            'api': f"{self.base_url}/api/v1/chat/completions",
            'v1': f"{self.base_url}/v1/chat/completions",
            'deepseek': f"{self.base_url}/deepseek/v1/chat/completions",
            'lingyiwanwu': f"{self.base_url}/lingyiwanwu/v1/chat/completions",
            'ollama': f"{self.base_url}/ollama/v1/chat/completions",
            'forward': f"{self.base_url}/forward/v1/chat/completions"
        }
        
        url = endpoint_map.get(service, endpoint_map['api'])
        
        payload = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful AI assistant testing streaming responses."
                },
                {
                    "role": "user", 
                    "content": f"Please write a short 50-word story about AI testing. Stream this response token by token via {service} endpoint."
                }
            ],
            "stream": True,  # å¯ç”¨æµå¼å“åº”
            "max_tokens": 150,
            "temperature": 0.8
        }
        
        headers = {
            "Content-Type": "application/json",
            "Accept": "text/event-stream",
            "User-Agent": "NGINX-AI-Gateway-Test/1.0"
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                start_time = time.time()
                
                async with session.post(url, json=payload, headers=headers, timeout=60) as response:
                    if response.status == 200:
                        self.log(f"ğŸŒŠ Streaming response started ({service})", "STREAM")
                        
                        chunks_received = 0
                        content_parts = []
                        
                        # è¯»å–æµå¼å“åº”
                        async for line in response.content:
                            line = line.decode('utf-8').strip()
                            
                            if line.startswith('data: '):
                                data_str = line[6:]  # ç§»é™¤ 'data: ' å‰ç¼€
                                
                                if data_str == '[DONE]':
                                    self.log("ğŸ Stream completed", "STREAM")
                                    break
                                
                                try:
                                    chunk_data = json.loads(data_str)
                                    delta = chunk_data.get('choices', [{}])[0].get('delta', {})
                                    content = delta.get('content', '')
                                    
                                    if content:
                                        content_parts.append(content)
                                        chunks_received += 1
                                        
                                        # æ˜¾ç¤ºæµå¼å†…å®¹ï¼ˆæ¯10ä¸ªå—æ˜¾ç¤ºä¸€æ¬¡ï¼‰
                                        if chunks_received % 10 == 0:
                                            self.log(f"ğŸ“¦ Received {chunks_received} chunks...", "STREAM")
                                            
                                except json.JSONDecodeError:
                                    continue
                        
                        response_time = time.time() - start_time
                        full_content = ''.join(content_parts)
                        
                        self.log(f"âœ… Streaming chat successful ({service})", "SUCCESS")
                        self.log(f"   Total chunks: {chunks_received}", "SUCCESS")
                        self.log(f"   Response time: {response_time:.2f}s", "SUCCESS")
                        self.log(f"   Content: {full_content[:100]}...", "SUCCESS")
                        
                        return chunks_received > 0
                        
                    else:
                        self.log(f"âŒ Streaming chat failed ({service}): {response.status}", "ERROR")
                        error_text = await response.text()
                        self.log(f"   Response: {error_text[:200]}", "ERROR")
                        return False
                        
        except Exception as e:
            self.log(f"âŒ Streaming chat error ({service}): {e}", "ERROR")
            return False
    
    def test_concurrent_requests(self, num_requests: int = 5) -> Dict[str, int]:
        """æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†èƒ½åŠ›"""
        self.log(f"ğŸš€ Testing Concurrent Requests ({num_requests} requests)...")
        
        def make_request(request_id: int) -> bool:
            """å‘é€å•ä¸ªè¯·æ±‚"""
            try:
                payload = {
                    "model": "gpt-3.5-turbo",
                    "messages": [
                        {"role": "user", "content": f"This is concurrent request #{request_id}. Please respond briefly."}
                    ],
                    "stream": False,
                    "max_tokens": 50
                }
                
                response = requests.post(
                    f"{self.base_url}/api/v1/chat/completions", 
                    json=payload, 
                    timeout=30
                )
                
                success = response.status_code == 200
                if success:
                    self.log(f"âœ… Request {request_id} completed successfully", "SUCCESS")
                else:
                    self.log(f"âŒ Request {request_id} failed: {response.status_code}", "ERROR")
                
                return success
                
            except Exception as e:
                self.log(f"âŒ Request {request_id} error: {e}", "ERROR")
                return False
        
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œå¹¶å‘è¯·æ±‚
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=num_requests) as executor:
            futures = [executor.submit(make_request, i+1) for i in range(num_requests)]
            results = [future.result() for future in futures]
        
        total_time = time.time() - start_time
        successful = sum(results)
        failed = len(results) - successful
        
        self.log(f"ğŸ“Š Concurrent Test Results:", "SUCCESS")
        self.log(f"   Total requests: {num_requests}", "SUCCESS")
        self.log(f"   Successful: {successful}", "SUCCESS")
        self.log(f"   Failed: {failed}", "SUCCESS")
        self.log(f"   Total time: {total_time:.2f}s", "SUCCESS")
        self.log(f"   Requests/second: {num_requests/total_time:.2f}", "SUCCESS")
        
        return {
            'total': num_requests,
            'successful': successful,
            'failed': failed,
            'total_time': total_time,
            'rps': num_requests/total_time
        }
    
    def test_rate_limiting(self) -> bool:
        """æµ‹è¯•é€Ÿç‡é™åˆ¶åŠŸèƒ½"""
        self.log("â±ï¸  Testing Rate Limiting...")
        
        # å¿«é€Ÿå‘é€å¤šä¸ªè¯·æ±‚ä»¥è§¦å‘é€Ÿç‡é™åˆ¶
        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": "Rate limit test"}],
            "stream": False,
            "max_tokens": 10
        }
        
        rate_limited = False
        successful_requests = 0
        
        for i in range(10):  # å‘é€10ä¸ªå¿«é€Ÿè¯·æ±‚
            try:
                response = requests.post(
                    f"{self.base_url}/api/v1/chat/completions",
                    json=payload,
                    timeout=5
                )
                
                if response.status_code == 429:  # Too Many Requests
                    self.log(f"âœ… Rate limiting triggered at request {i+1}", "SUCCESS")
                    rate_limited = True
                    break
                elif response.status_code == 200:
                    successful_requests += 1
                    time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿ
                    
            except Exception as e:
                self.log(f"   Request {i+1} error: {e}", "WARNING")
        
        if rate_limited:
            self.log("âœ… Rate limiting is working correctly", "SUCCESS")
        else:
            self.log(f"âš ï¸  Rate limiting not triggered ({successful_requests} successful)", "WARNING")
            
        return rate_limited
    
    def test_external_access(self, external_url: str = None) -> bool:
        """æµ‹è¯•å¤–éƒ¨è®¿é—®èƒ½åŠ›"""
        if not external_url:
            self.log("ğŸŒ Skipping external access test (no external URL provided)", "WARNING")
            return False
            
        self.log(f"ğŸŒ Testing External Access: {external_url}")
        
        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {"role": "user", "content": "This is an external access test. Please confirm connectivity."}
            ],
            "stream": False,
            "max_tokens": 50
        }
        
        try:
            response = requests.post(
                f"{external_url}/api/v1/chat/completions",
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                self.log("âœ… External access successful", "SUCCESS")
                return True
            else:
                self.log(f"âŒ External access failed: {response.status_code}", "ERROR")
                return False
                
        except Exception as e:
            self.log(f"âŒ External access error: {e}", "ERROR")
            return False
    
    def test_all_ai_services(self) -> Dict[str, bool]:
        """æµ‹è¯•æ‰€æœ‰ AI æœåŠ¡ç«¯ç‚¹"""
        self.log("ğŸ¯ Testing All AI Service Endpoints...")
        
        services = {
            'api': 'gpt-3.5-turbo',
            'v1': 'gpt-3.5-turbo', 
            'deepseek': 'deepseek-chat',
            'lingyiwanwu': 'yi-34b-chat',
            'ollama': 'llama3.2',
            'forward': 'gpt-3.5-turbo'
        }
        
        results = {}
        for service, model in services.items():
            try:
                # æµ‹è¯•éæµå¼
                non_stream_result = self.test_non_streaming_chat(service, model)
                results[f"{service}_non_stream"] = non_stream_result
                
                time.sleep(1)  # é˜²æ­¢é€Ÿç‡é™åˆ¶
                
            except Exception as e:
                self.log(f"âŒ Service {service} test failed: {e}", "ERROR")
                results[f"{service}_non_stream"] = False
        
        return results
    
    async def test_all_streaming_services(self) -> Dict[str, bool]:
        """æµ‹è¯•æ‰€æœ‰æœåŠ¡çš„æµå¼å“åº”"""
        self.log("ğŸŒŠ Testing Streaming for All Services...")
        
        services = {
            'api': 'gpt-3.5-turbo',
            'ollama': 'llama3.2'
        }
        
        results = {}
        for service, model in services.items():
            try:
                stream_result = await self.test_streaming_chat(service, model)
                results[f"{service}_stream"] = stream_result
                await asyncio.sleep(2)  # é˜²æ­¢é€Ÿç‡é™åˆ¶
                
            except Exception as e:
                self.log(f"âŒ Streaming test {service} failed: {e}", "ERROR")
                results[f"{service}_stream"] = False
        
        return results
    
    def generate_test_report(self, results: Dict) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = f"""
==============================================================================
                    ğŸ¯ NGINX AI Gateway Test Report
==============================================================================
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Gateway URL: {self.base_url}

ğŸ“Š Test Summary:
"""
        
        total_tests = len(results)
        passed_tests = sum(1 for result in results.values() if result)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        report += f"   Total Tests: {total_tests}\n"
        report += f"   Passed: {passed_tests}\n"
        report += f"   Failed: {total_tests - passed_tests}\n"
        report += f"   Success Rate: {success_rate:.1f}%\n\n"
        
        report += "ğŸ“‹ Detailed Results:\n"
        for test_name, result in results.items():
            status = "âœ… PASS" if result else "âŒ FAIL"
            report += f"   {test_name}: {status}\n"
        
        report += "\n"
        report += "ğŸ”§ Recommendations:\n"
        
        if not results.get('nginx_health', False):
            report += "   â€¢ Check NGINX gateway configuration and restart service\n"
        
        if not any(k.endswith('_stream') and v for k, v in results.items()):
            report += "   â€¢ Verify streaming support in backend services\n"
            report += "   â€¢ Check NGINX proxy_buffering settings\n"
        
        if results.get('concurrent_failed', 0) > 0:
            report += "   â€¢ Consider increasing worker processes or connection limits\n"
        
        report += "\n=============================================================================="
        
        return report
    
    async def run_comprehensive_test(self, external_url: str = None) -> Dict[str, bool]:
        """è¿è¡Œå…¨é¢çš„æµ‹è¯•å¥—ä»¶"""
        self.log("ğŸš€ Starting Comprehensive NGINX AI Gateway Test...", "SUCCESS")
        self.log(f"ğŸ¯ Target Gateway: {self.base_url}", "SUCCESS")
        self.log("=" * 80, "SUCCESS")
        
        all_results = {}
        
        # 1. åŸºç¡€å¥åº·æ£€æŸ¥
        all_results['nginx_health'] = self.test_nginx_health()
        if not all_results['nginx_health']:
            self.log("âŒ NGINX Gateway unavailable - stopping tests", "ERROR")
            return all_results
        
        # 2. æœåŠ¡å¥åº·æ£€æŸ¥
        service_health = self.test_service_health()
        all_results.update(service_health)
        
        # 3. æµ‹è¯•æ‰€æœ‰AIæœåŠ¡ç«¯ç‚¹
        service_results = self.test_all_ai_services()
        all_results.update(service_results)
        
        # 4. æµ‹è¯•æµå¼å“åº”
        streaming_results = await self.test_all_streaming_services()
        all_results.update(streaming_results)
        
        # 5. å¹¶å‘æµ‹è¯•
        concurrent_results = self.test_concurrent_requests(5)
        all_results['concurrent_successful'] = concurrent_results['successful'] >= 4
        all_results['concurrent_failed'] = concurrent_results['failed']
        
        # 6. é€Ÿç‡é™åˆ¶æµ‹è¯•
        all_results['rate_limiting'] = self.test_rate_limiting()
        
        # 7. å¤–éƒ¨è®¿é—®æµ‹è¯•ï¼ˆå¦‚æœæä¾›äº†å¤–éƒ¨URLï¼‰
        if external_url:
            all_results['external_access'] = self.test_external_access(external_url)
        
        # ç”Ÿæˆå¹¶æ˜¾ç¤ºæŠ¥å‘Š
        self.log("=" * 80, "SUCCESS")
        report = self.generate_test_report(all_results)
        print(report)
        
        return all_results


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='NGINX AI Gateway Service Tester')
    parser.add_argument('--url', default='http://localhost', 
                       help='NGINX Gateway URL (default: http://localhost)')
    parser.add_argument('--external-url', 
                       help='External URL for testing external access')
    parser.add_argument('--quick', action='store_true',
                       help='Run only basic tests')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = NGINXAIServiceTester(args.url)
    
    if args.quick:
        # å¿«é€Ÿæµ‹è¯•
        tester.log("âš¡ Running Quick Test Suite...")
        results = {
            'nginx_health': tester.test_nginx_health(),
            'api_non_stream': tester.test_non_streaming_chat('api'),
            'api_stream': await tester.test_streaming_chat('api')
        }
    else:
        # å…¨é¢æµ‹è¯•
        results = await tester.run_comprehensive_test(args.external_url)
    
    # è¿”å›é€‚å½“çš„é€€å‡ºä»£ç 
    success_rate = sum(results.values()) / len(results) if results else 0
    sys.exit(0 if success_rate >= 0.8 else 1)


if __name__ == "__main__":
    # å®‰è£…å¿…è¦çš„åŒ…
    try:
        import aiohttp
        import requests
    except ImportError:
        print("Installing required packages...")
        import subprocess
        subprocess.check_call([sys.executable, "-m", "pip", "install", "aiohttp", "requests"])
        import aiohttp
        import requests
    
    asyncio.run(main()) 