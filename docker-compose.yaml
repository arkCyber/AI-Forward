# ==============================================================================
# Docker Compose Configuration for OpenAI Forward + Ollama + AI Router + WebUI
# ==============================================================================
# Description: Multi-AI service stack with cloud and local AI providers and WebUI
# Author: Assistant
# Created: 2024-12-19
# Updated: 2024-12-19
# Version: 3.1
#
# Services included:
# - Smart AI Router (Unified API endpoint on port 9000)
# - OpenAI Forward (DeepSeek + LingyiWanwu + Ollama proxy)
# - OpenAI Forward WebUI (Configuration management interface on port 8001)
# - Ollama (Local AI models)
# ==============================================================================

services:
  # NGINX Reverse Proxy and Load Balancer - Multi-User Gateway
  nginx:
    image: nginx:1.25-alpine
    container_name: nginx-gateway
    
    ports:
      - "80:80"      # HTTP access port
      - "443:443"    # HTTPS access port (ready for SSL)
    
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./logs/nginx:/var/log/nginx
      - ./static:/usr/share/nginx/html/static:ro
    
    environment:
      - TZ=Asia/Shanghai
      - NGINX_WORKER_PROCESSES=auto
      - NGINX_WORKER_CONNECTIONS=4096
    
    # NGINX depends on all backend services
    depends_on:
      ai-router:
        condition: service_healthy
      openai-forward:
        condition: service_started
      openai-forward-webui:
        condition: service_healthy
      ollama:
        condition: service_healthy
    
    restart: unless-stopped
    
    # NGINX health check
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.2'
        reservations:
          memory: 128M
          cpus: '0.1'
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=nginx-gateway"
    
    networks:
      - ai-services-network

  # Smart AI Router - Unified API Endpoint
  ai-router:
    build:
      context: .
      dockerfile: Dockerfile.router
    image: ai-router:latest
    container_name: smart-ai-router
    
    ports:
      - "9000:9000"  # ç»Ÿä¸€APIç«¯ç‚¹
    
    volumes:
      - ./ai-router-logs:/app/logs
    
    environment:
      - TZ=Asia/Shanghai
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
    
    # ä¾èµ–OpenAI ForwardæœåŠ¡
    depends_on:
      openai-forward:
        condition: service_started
    
    restart: unless-stopped
    
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.3'
        reservations:
          memory: 256M
          cpus: '0.1'
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=ai-router"
    
    networks:
      - ai-services-network

  # OpenAI Forward Proxy Service
  openai-forward:
    build: 
      context: .
      dockerfile: Dockerfile
    image: openai-forward:latest
    container_name: openai-forward-proxy
    
    ports:
      - "8000:8000"
    
    volumes:
      - ./openai-forward-config.yaml:/home/openai-forward/openai-forward-config.yaml:ro
      - ./logs:/home/openai-forward/logs
      - ./FLAXKV_DB:/home/openai-forward/FLAXKV_DB
    
    environment:
      - TZ=Asia/Shanghai
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - OLLAMA_HOST=ollama:11434  # å¯ç”¨Ollamaè¿æ¥
    
    # å¯ç”¨Ollamaä¾èµ–
    depends_on:
      ollama:
        condition: service_healthy
    
    restart: unless-stopped
    
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=openai-forward"
    
    networks:
      - ai-services-network

  # OpenAI Forward WebUI Service - Configuration Management Interface
  openai-forward-webui:
    build:
      context: .
      dockerfile: webui.Dockerfile
    image: openai-forward-webui:latest
    container_name: openai-forward-webui
    
    ports:
      - "8001:8001"  # WebUIç®¡ç†ç•Œé¢ç«¯å£
    
    volumes:
      - ./openai-forward-config.yaml:/home/openai-forward/openai-forward-config.yaml
      - ./logs:/home/openai-forward/logs
      - ./webui-logs:/home/openai-forward/webui-logs
      - ./FLAXKV_DB:/home/openai-forward/FLAXKV_DB
    
    environment:
      - TZ=Asia/Shanghai
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - OPENAI_FORWARD_HOST=openai-forward  # è¿æ¥åˆ°OpenAI ForwardæœåŠ¡
      - WEBUI_RESTART_PORT=15555
      - WEBUI_LOG_PORT=15556
      - STREAMLIT_SERVER_PORT=8001
      - STREAMLIT_SERVER_HEADLESS=true
    
    # ä¾èµ–OpenAI ForwardæœåŠ¡
    depends_on:
      openai-forward:
        condition: service_started
    
    restart: unless-stopped
    
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.3'
        reservations:
          memory: 256M
          cpus: '0.1'
    
    # WebUIå¥åº·æ£€æŸ¥
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=openai-forward-webui"
    
    networks:
      - ai-services-network

  # Ollama Local AI Service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    
    ports:
      - "11434:11434"  # Ollama APIç«¯å£
    
    volumes:
      - ollama-models:/root/.ollama  # æ¨¡å‹æŒä¹…åŒ–å­˜å‚¨
      - ./ollama-logs:/var/log/ollama  # æ—¥å¿—ç›®å½•
    
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODELS=/root/.ollama/models
      - OLLAMA_KEEP_ALIVE=24h
      - TZ=Asia/Shanghai
    
    # Ollamaå¥åº·æ£€æŸ¥
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    
    restart: unless-stopped
    
    # Ollamaéœ€è¦æ›´å¤šèµ„æºæ¥è¿è¡ŒAIæ¨¡å‹
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    
    # å¦‚æœæœ‰NVIDIA GPUï¼Œå–æ¶ˆæ³¨é‡Šä»¥ä¸‹é…ç½®
    # runtime: nvidia
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all
    #   - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=ollama"
    
    networks:
      - ai-services-network

networks:
  ai-services-network:
    driver: bridge
    name: ai-services-net
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  logs:
    driver: local
  cache:
    driver: local
  ai-router-logs:
    driver: local
  webui-logs:
    driver: local
  ollama-models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./ollama-models  # æœ¬åœ°æ¨¡å‹å­˜å‚¨ç›®å½•

# ==============================================================================
# ä½¿ç”¨è¯´æ˜:
#
# 1. å¯åŠ¨æ‰€æœ‰æœåŠ¡:
#    docker-compose up -d
#
# 2. ä¸‹è½½å¹¶è¿è¡ŒOllamaæ¨¡å‹:
#    docker exec -it ollama-server ollama pull llama3.2
#    docker exec -it ollama-server ollama pull qwen2:7b
#    docker exec -it ollama-server ollama pull codegemma
#
# 3. æµ‹è¯•æœåŠ¡ (é€šè¿‡NGINXç½‘å…³ - æ¨è):
#    # ä¸»è¦APIç«¯ç‚¹ (è´Ÿè½½å‡è¡¡ + é€Ÿç‡é™åˆ¶):
#    curl -X POST http://localhost/api/v1/chat/completions \
#      -H "Content-Type: application/json" \
#      -d '{"model":"gpt-3.5-turbo","messages":[{"role":"user","content":"Hello!"}]}'
#    
#    # å¥åº·æ£€æŸ¥å’Œç›‘æ§:
#    curl http://localhost/nginx-health    # NGINXçŠ¶æ€
#    curl http://localhost/health         # æœåŠ¡å¥åº·çŠ¶æ€  
#    curl http://localhost/stats          # ä½¿ç”¨ç»Ÿè®¡
#
# 4. æŸ¥çœ‹æ—¥å¿—:
#    docker-compose logs -f nginx
#    docker-compose logs -f ai-router
#    docker-compose logs -f openai-forward
#    docker-compose logs -f openai-forward-webui
#    docker-compose logs -f ollama
#
# 5. å¤šç”¨æˆ·è®¿é—®ç«¯ç‚¹ (é€šè¿‡NGINXç½‘å…³):
# 
# ğŸš€ ä¸»è¦ç”Ÿäº§ç«¯ç‚¹ (æ¨è):
# - ğŸ¯ ç»Ÿä¸€AI API: http://localhost/api/v1/chat/completions
# - ğŸ¯ WebUIç®¡ç†ç•Œé¢: http://localhost/webui/
# - ğŸ¯ å¥åº·æ£€æŸ¥: http://localhost/health
# - ğŸ¯ ä½¿ç”¨ç»Ÿè®¡: http://localhost/stats
# - ğŸ¯ NGINXçŠ¶æ€: http://localhost/nginx-health
#
# ğŸ”§ ç›´æ¥æœåŠ¡è®¿é—®:
# - DeepSeek API: http://localhost/deepseek/v1/chat/completions
# - LingyiWanwu API: http://localhost/lingyiwanwu/v1/chat/completions  
# - Ollama API: http://localhost/ollama/v1/chat/completions
# - OpenAI Forward: http://localhost/forward/v1/chat/completions
# - Ollamaç›´æ¥è®¿é—®: http://localhost/ollama-direct/
#
# ğŸ“Š å¤šç”¨æˆ·ç‰¹æ€§:
# - é€Ÿç‡é™åˆ¶: APIè°ƒç”¨ 100è¯·æ±‚/åˆ†é’Ÿ, èŠå¤© 60è¯·æ±‚/åˆ†é’Ÿ
# - è¿æ¥é™åˆ¶: æ¯IPæœ€å¤š50ä¸ªå¹¶å‘è¿æ¥
# - è´Ÿè½½å‡è¡¡: è‡ªåŠ¨æ•…éšœè½¬ç§»å’Œè´Ÿè½½åˆ†é…
# - å®‰å…¨å¤´: XSSä¿æŠ¤, å†…å®¹ç±»å‹å—…æ¢ä¿æŠ¤ç­‰
# - å‹ç¼©ä¼˜åŒ–: Gzipå‹ç¼©å‡å°‘ä¼ è¾“å¤§å°
# - è¿æ¥æ± : HTTP/1.1 keepaliveä¼˜åŒ–æ€§èƒ½
#
# ğŸ›ï¸ ç›´æ¥è®¿é—®ç«¯å£ (å¼€å‘è°ƒè¯•ç”¨):
# - AI Router: http://localhost:9000
# - OpenAI Forward: http://localhost:8000  
# - WebUI: http://localhost:8001
# - Ollama: http://localhost:11434
# ==============================================================================
