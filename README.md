# ğŸš€ AI-Forward

<div align="center">

![GitHub stars](https://img.shields.io/github/stars/arkCyber/AI-Forward?style=social)
![GitHub forks](https://img.shields.io/github/forks/arkCyber/AI-Forward?style=social)
![GitHub issues](https://img.shields.io/github/issues/arkCyber/AI-Forward)
![GitHub license](https://img.shields.io/github/license/arkCyber/AI-Forward)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)
![NGINX](https://img.shields.io/badge/nginx-%23009639.svg?style=flat&logo=nginx&logoColor=white)

**ğŸŒŸ Enterprise-Grade Multi-AI Gateway with NGINX Load Balancing**

*A high-performance, scalable AI service proxy supporting multiple providers*

[ğŸš€ Quick Start](#quick-start) â€¢ 
[ğŸ“– Documentation](#documentation) â€¢ 
[ğŸ¯ Features](#features) â€¢ 
[ğŸ› Issues](https://github.com/arkCyber/AI-Forward/issues) â€¢ 
[ğŸ¤ Contributing](#contributing)

</div>

---

## ğŸ¯ Features

### ğŸŒ Multi-AI Provider Support
- **ğŸ¤– DeepSeek AI** - Advanced reasoning and coding capabilities
- **ğŸ§  LingyiWanwu (é›¶ä¸€ä¸‡ç‰©)** - Chinese-optimized language models  
- **ğŸ¦™ Ollama** - Local open-source models (Llama, Qwen, CodeGemma)
- **ğŸ”¥ OpenAI** - GPT models support
- **â• Extensible** - Easy to add new providers

### ğŸš€ Enterprise-Grade Infrastructure
- **âš¡ NGINX Load Balancer** - High-performance reverse proxy
- **ğŸ›¡ï¸ Rate Limiting** - Multi-tier rate limiting (100 API/min, 60 chat/min)
- **ğŸ“Š Real-time Monitoring** - Health checks and performance metrics
- **ğŸ”„ Auto Failover** - Automatic service recovery and routing
- **ğŸ“¦ Docker Containerized** - Easy deployment and scaling

### ğŸ›ï¸ Management & Monitoring
- **ğŸ–¥ï¸ WebUI Dashboard** - Visual configuration and monitoring
- **ğŸ“ˆ Load Testing** - Built-in multi-user testing tools
- **ğŸ” System Monitor** - Real-time service status dashboard
- **ğŸ“Š Statistics** - Usage analytics and performance metrics

### ğŸ”’ Security & Performance
- **ğŸ” API Key Management** - Secure authentication system
- **ğŸ—œï¸ Compression** - Gzip compression for bandwidth optimization
- **âš¡ Connection Pooling** - HTTP/1.1 keepalive optimization
- **ğŸ›¡ï¸ Security Headers** - XSS protection, content type validation

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.10+ (for monitoring tools)
- 4GB+ RAM recommended

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/arkCyber/AI-Forward.git
cd AI-Forward
```

### 2ï¸âƒ£ Configure Services
```bash
# Copy and edit configuration
cp openai-forward-config.example.yaml openai-forward-config.yaml
# Edit with your API keys
```

### 3ï¸âƒ£ Start All Services
```bash
# Start the complete stack
docker-compose up -d

# Check service status
docker-compose ps
```

### 4ï¸âƒ£ Access Services
- **ğŸ¯ Main API**: `http://localhost/api/v1/chat/completions`
- **ğŸ–¥ï¸ WebUI**: `http://localhost/webui/`
- **ğŸ“Š Health Check**: `http://localhost/health`
- **ğŸ“ˆ Statistics**: `http://localhost/stats`

---

## ğŸ“– Documentation

### ğŸ”§ API Usage

#### Unified API Endpoint (Recommended)
```bash
curl -X POST http://localhost/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "deepseek-chat",
    "messages": [{"role": "user", "content": "Hello!"}],
    "max_tokens": 100
  }'
```

#### Service-Specific Endpoints
```bash
# DeepSeek AI
curl -X POST http://localhost/deepseek/v1/chat/completions

# LingyiWanwu (é›¶ä¸€ä¸‡ç‰©)  
curl -X POST http://localhost/lingyiwanwu/v1/chat/completions

# Ollama Local Models
curl -X POST http://localhost/ollama/v1/chat/completions

# OpenAI
curl -X POST http://localhost/forward/v1/chat/completions
```

### ğŸ³ Docker Services

| Service | Port | Description |
|---------|------|-------------|
| NGINX Gateway | 80, 443 | Load balancer & reverse proxy |
| AI Router | 9000 | Unified API endpoint |
| OpenAI Forward | 8000 | Multi-provider proxy |
| WebUI | 8001 | Management interface |
| Ollama | 11434 | Local AI models |

### ğŸ“Š Monitoring & Testing

#### Real-time System Monitor
```bash
# Install dependencies
pip install aiohttp

# Start monitoring dashboard
python monitor_system.py
```

#### Multi-user Load Testing
```bash
# Run load test with 10 concurrent users
python test_multiuser.py
```

---

## ğŸ› ï¸ Management Commands

```bash
# View all services
docker-compose ps

# View logs
docker-compose logs -f nginx
docker-compose logs -f ai-router

# Restart services
docker-compose restart

# Stop all services
docker-compose down

# Download Ollama models
docker exec -it ollama-server ollama pull llama3.2:1b
docker exec -it ollama-server ollama pull qwen2:1.5b
```

---

## ğŸ—ï¸ Architecture

```
Internet/Users â†’ NGINX Gateway (Port 80/443) â†’ Backend Services
                     â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚     Load Balancing              â”‚
          â”‚     Rate Limiting               â”‚
          â”‚     SSL Termination             â”‚
          â”‚     Security Headers            â”‚
          â”‚     Compression                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   AI Router    â”‚ OpenAI Forward â”‚     WebUI       â”‚
    â”‚   (Port 9000)  â”‚  (Port 8000)   â”‚  (Port 8001)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Ollama    â”‚
                â”‚ (Port 11434)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Performance Features

### ğŸš€ Multi-User Capabilities
- **Rate Limiting**: 100 API calls/min, 60 chat/min per IP
- **Connection Limits**: Max 50 concurrent connections per IP  
- **Load Balancing**: Automatic request distribution
- **Auto Scaling**: Easy horizontal scaling support

### âš¡ Optimization
- **Gzip Compression**: Reduces bandwidth by 60-80%
- **Connection Pooling**: HTTP/1.1 keepalive optimization
- **Caching**: Intelligent response caching (FlaxKV)
- **Async Processing**: High-concurrency request handling

---

## ğŸ”§ Configuration

### Key Configuration Files
- `nginx.conf` - NGINX load balancer settings
- `docker-compose.yaml` - Service orchestration
- `openai-forward-config.yaml` - AI provider configuration
- `.env` - Environment variables

### Environment Variables
```bash
# API Keys
DEEPSEEK_API_KEY=your_deepseek_key
LINGYIWANWU_API_KEY=your_lingyiwanwu_key
OPENAI_API_KEY=your_openai_key

# Service Configuration
NGINX_WORKER_PROCESSES=auto
LOG_LEVEL=INFO
```

---

## ğŸš€ Production Deployment

### ğŸ”’ Security Recommendations
1. **Enable HTTPS** - Add SSL certificates
2. **Firewall Rules** - Restrict access to necessary ports
3. **API Key Rotation** - Regular key updates
4. **Access Logs** - Enable comprehensive logging

### ğŸ“Š Monitoring Setup
1. **Prometheus + Grafana** - Metrics collection
2. **ELK Stack** - Log analysis
3. **Alerting** - Service health alerts
4. **Backup Strategy** - Configuration and data backup

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### ğŸ› Bug Reports
- Use [GitHub Issues](https://github.com/arkCyber/AI-Forward/issues)
- Include system info and error logs
- Provide reproduction steps

### ğŸ’¡ Feature Requests
- Open a GitHub Issue with the `enhancement` label
- Describe the use case and expected behavior
- Consider submitting a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [OpenAI Forward](https://github.com/KenyonY/openai-forward) - Original project base
- [NGINX](https://nginx.org/) - High-performance web server
- [Ollama](https://ollama.ai/) - Local AI model runtime
- [Docker](https://docker.com/) - Containerization platform

---

## ğŸ“ Support

- ğŸ“§ Email: arksong2018@gmail.com
- ğŸ› Issues: [GitHub Issues](https://github.com/arkCyber/AI-Forward/issues)
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/arkCyber/AI-Forward/discussions)

---

<div align="center">

**â­ Star this repository if you find it helpful!**

Made with â¤ï¸ by [arkSong](https://github.com/arkSong)

</div>
