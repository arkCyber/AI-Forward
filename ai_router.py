#!/usr/bin/env python3

# ==============================================================================
# Smart AI Router Service with Low-Level ASGI Streaming
# ==============================================================================
# Description: Unified API endpoint with intelligent load balancing and real-time streaming
# Author: Assistant
# Created: 2024-12-19
# Version: 1.2 - Added low-level ASGI streaming for real-time data forwarding
#
# Features:
# - Unified OpenAI-compatible API endpoint
# - Random/weighted selection of AI providers
# - Automatic failover and retry mechanism
# - Request logging and performance monitoring
# - Health checking of backend services
# - API Key authentication and user quota management
# - Low-level ASGI streaming for zero-buffering real-time data forwarding
# ==============================================================================

import asyncio
import random
import time
import logging
import json
from datetime import datetime
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
from contextlib import asynccontextmanager

import httpx
from fastapi import FastAPI, HTTPException, Request, Header, Depends
from fastapi.responses import StreamingResponse, JSONResponse, Response
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
import uvicorn

# Configure logging with timestamp
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("ai_router")

class ProviderStatus(Enum):
    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    UNKNOWN = "unknown"

@dataclass
class AIProvider:
    """AI Provider configuration"""
    name: str
    base_url: str
    api_key: str
    models: List[str]
    weight: int = 1  # æƒé‡ï¼Œæ•°å€¼è¶Šå¤§è¢«é€‰ä¸­æ¦‚ç‡è¶Šé«˜
    status: ProviderStatus = ProviderStatus.UNKNOWN
    last_check: float = 0
    response_time: float = 0
    error_count: int = 0

@dataclass
class UserInfo:
    """User information and quota management"""
    user_id: str
    api_key: str
    daily_limit: int = 1000  # æ¯æ—¥è¯·æ±‚é™åˆ¶
    requests_today: int = 0
    total_requests: int = 0
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())
    last_request: str = field(default_factory=lambda: datetime.now().isoformat())
    is_active: bool = True

class ChatRequest(BaseModel):
    """OpenAI Chat Completion Request"""
    model: str
    messages: List[Dict[str, str]]
    max_tokens: Optional[int] = 150
    temperature: Optional[float] = 0.7
    stream: Optional[bool] = True  # é»˜è®¤å¯ç”¨æµå¼è¾“å‡º
    x_use_asgi_streaming: Optional[bool] = False  # ä½¿ç”¨ä½å±‚ASGIæµå¤„ç†
    # å…¶ä»–OpenAIå‚æ•°...

class SmartAIRouter:
    """Smart AI Router with load balancing and failover"""
    
    def __init__(self):
        """Initialize the AI router with multiple providers"""
        self.providers = [
            AIProvider(
                name="deepseek",
                base_url="http://openai-forward-proxy:8000/deepseek/v1",
                api_key="sk-878a5319c7b14bc48109e19315361b7f",
                models=["deepseek-chat", "deepseek-coder"],
                weight=3  # é«˜æƒé‡ï¼Œä¸“ä¸šç¼–ç¨‹
            ),
            AIProvider(
                name="lingyiwanwu",
                base_url="http://openai-forward-proxy:8000/lingyiwanwu/v1", 
                api_key="72ebf8a6191e45bab0f646659c8cb121",
                models=["yi-34b-chat", "yi-6b-chat"],
                weight=3  # é«˜æƒé‡ï¼Œä¸­æ–‡ä¼˜åŒ–
            ),
            AIProvider(
                name="ollama",
                base_url="http://openai-forward-proxy:8000/ollama/v1",
                api_key="ollama-local-key",
                models=["llama3.2", "qwen2:7b", "codegemma"],
                weight=2  # ä¸­ç­‰æƒé‡ï¼Œæœ¬åœ°æ¨¡å‹
            )
        ]
        
        self.client = httpx.AsyncClient(timeout=60.0)
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "provider_usage": {provider.name: 0 for provider in self.providers}
        }
        
        # å¥åº·æ£€æŸ¥ä»»åŠ¡å¼•ç”¨ï¼ˆå°†åœ¨startupæ—¶åˆ›å»ºï¼‰
        self.health_check_task = None
        
        # ç”¨æˆ·ç®¡ç†å’ŒAPIå¯†é’¥é…ç½®
        self.setup_authentication()
    
    def setup_authentication(self):
        """Setup authentication configuration"""
        # é…ç½®é€‰é¡¹ï¼šå…±äº«å¯†é’¥æ¨¡å¼ vs å¤šç”¨æˆ·æ¨¡å¼
        self.auth_mode = "shared"  # "shared" æˆ– "multi_user"
        
        # å…±äº«æ¨¡å¼ï¼šæ‰€æœ‰ç”¨æˆ·ä½¿ç”¨åŒä¸€ä¸ªå¯†é’¥
        self.shared_api_key = "sk-8d6804b011614dba7bd065f8644514b"
        
        # å¤šç”¨æˆ·æ¨¡å¼ï¼šç”¨æˆ·æ•°æ®åº“ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨çœŸå®æ•°æ®åº“ï¼‰
        self.users_db = {
            "sk-user-demo-key-001": UserInfo(
                user_id="demo_user_001",
                api_key="sk-user-demo-key-001",
                daily_limit=500
            ),
            "sk-user-admin-key-999": UserInfo(
                user_id="admin_user",
                api_key="sk-user-admin-key-999", 
                daily_limit=10000
            ),
            # å¯ä»¥æ·»åŠ æ›´å¤šç”¨æˆ·...
        }
        
        logger.info(f"ğŸ” Authentication mode: {self.auth_mode}")
        if self.auth_mode == "shared":
            logger.info(f"ğŸ”‘ Shared API key: {self.shared_api_key}")
        else:
            logger.info(f"ğŸ‘¥ Multi-user mode: {len(self.users_db)} users configured")
    
    def verify_api_key(self, api_key: str) -> Optional[UserInfo]:
        """Verify API key and return user info"""
        if self.auth_mode == "shared":
            if api_key == self.shared_api_key:
                # å…±äº«æ¨¡å¼è¿”å›é»˜è®¤ç”¨æˆ·ä¿¡æ¯
                return UserInfo(
                    user_id="shared_user",
                    api_key=api_key,
                    daily_limit=999999  # æ— é™åˆ¶
                )
            else:
                return None
        
        elif self.auth_mode == "multi_user":
            return self.users_db.get(api_key)
        
        return None
    
    def check_quota(self, user_info: UserInfo) -> bool:
        """Check if user has remaining quota"""
        today = datetime.now().date().isoformat()
        
        # å¦‚æœæ˜¯æ–°çš„ä¸€å¤©ï¼Œé‡ç½®è®¡æ•°
        if user_info.last_request[:10] != today:
            user_info.requests_today = 0
        
        return user_info.requests_today < user_info.daily_limit
    
    def update_user_stats(self, user_info: UserInfo):
        """Update user usage statistics"""
        user_info.requests_today += 1
        user_info.total_requests += 1
        user_info.last_request = datetime.now().isoformat()

    async def start_health_checks(self):
        """å¯åŠ¨å¥åº·æ£€æŸ¥ä»»åŠ¡"""
        if self.health_check_task is None:
            self.health_check_task = asyncio.create_task(self.health_check_loop())
            logger.info("ğŸ” Health check task started")
    
    async def health_check_provider(self, provider: AIProvider) -> bool:
        """Check if a provider is healthy"""
        try:
            start_time = time.time()
            
            # ç®€å•çš„å¥åº·æ£€æŸ¥è¯·æ±‚
            test_request = {
                "model": provider.models[0],
                "messages": [{"role": "user", "content": "test"}],
                "max_tokens": 1
            }
            
            response = await self.client.post(
                f"{provider.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {provider.api_key}",
                    "Content-Type": "application/json"
                },
                json=test_request,
                timeout=10.0
            )
            
            provider.response_time = time.time() - start_time
            provider.last_check = time.time()
            
            if response.status_code == 200:
                provider.status = ProviderStatus.HEALTHY
                provider.error_count = 0
                logger.info(f"âœ… Provider {provider.name} is healthy (response_time: {provider.response_time:.2f}s)")
                return True
            else:
                provider.status = ProviderStatus.UNHEALTHY
                provider.error_count += 1
                logger.warning(f"âš ï¸ Provider {provider.name} returned status {response.status_code}")
                return False
                
        except Exception as e:
            provider.status = ProviderStatus.UNHEALTHY
            provider.error_count += 1
            provider.last_check = time.time()
            logger.error(f"âŒ Provider {provider.name} health check failed: {str(e)}")
            return False
    
    async def health_check_loop(self):
        """Periodic health check for all providers"""
        while True:
            try:
                logger.info("ğŸ” Starting health check for all providers...")
                
                tasks = [self.health_check_provider(provider) for provider in self.providers]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                healthy_count = sum(1 for provider in self.providers if provider.status == ProviderStatus.HEALTHY)
                logger.info(f"ğŸ“Š Health check completed: {healthy_count}/{len(self.providers)} providers healthy")
                
                # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                await asyncio.sleep(30)
                
            except Exception as e:
                logger.error(f"âŒ Health check loop error: {str(e)}")
                await asyncio.sleep(30)
    
    def select_provider(self, request: ChatRequest) -> Optional[AIProvider]:
        """Smart provider selection with weighted random and failover"""
        
        # 1. è¿‡æ»¤å¥åº·çš„æä¾›å•†
        healthy_providers = [
            p for p in self.providers 
            if p.status == ProviderStatus.HEALTHY and p.error_count < 5
        ]
        
        if not healthy_providers:
            logger.warning("âš ï¸ No healthy providers available, using all providers as fallback")
            healthy_providers = self.providers
        
        # 2. æ ¹æ®æ¨¡å‹è¿‡æ»¤ï¼ˆå¦‚æœæŒ‡å®šäº†ç‰¹å®šæ¨¡å‹ï¼‰
        model_compatible_providers = []
        for provider in healthy_providers:
            # å¦‚æœè¯·æ±‚æŒ‡å®šäº†å…·ä½“æ¨¡å‹ï¼Œä¼˜å…ˆé€‰æ‹©æ”¯æŒè¯¥æ¨¡å‹çš„æä¾›å•†
            if request.model in provider.models:
                model_compatible_providers.append(provider)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ”¯æŒç‰¹å®šæ¨¡å‹çš„æä¾›å•†ï¼Œä½¿ç”¨æ‰€æœ‰å¥åº·çš„æä¾›å•†
        if not model_compatible_providers:
            model_compatible_providers = healthy_providers
        
        # 3. åŠ æƒéšæœºé€‰æ‹©
        if model_compatible_providers:
            # è®¡ç®—æƒé‡ï¼ˆè€ƒè™‘å“åº”æ—¶é—´å’Œé”™è¯¯ç‡ï¼‰
            weighted_providers = []
            for provider in model_compatible_providers:
                # åŸºç¡€æƒé‡
                weight = provider.weight
                
                # æ ¹æ®å“åº”æ—¶é—´è°ƒæ•´æƒé‡ï¼ˆå“åº”è¶Šå¿«æƒé‡è¶Šé«˜ï¼‰
                if provider.response_time > 0:
                    time_factor = max(0.1, 2.0 - provider.response_time)  # 2ç§’ä»¥å†…è¾ƒå¥½
                    weight *= time_factor
                
                # æ ¹æ®é”™è¯¯æ•°è°ƒæ•´æƒé‡
                error_factor = max(0.1, 1.0 - provider.error_count * 0.1)
                weight *= error_factor
                
                weighted_providers.extend([provider] * max(1, int(weight)))
            
            selected = random.choice(weighted_providers)
            logger.info(f"ğŸ¯ Selected provider: {selected.name} (weight: {selected.weight}, errors: {selected.error_count})")
            return selected
        
        logger.error("âŒ No suitable provider found")
        return None
    
    async def forward_request(self, provider: AIProvider, request: ChatRequest, request_start_time: float = None) -> Any:
        """Forward request to selected provider with streaming support"""
        if request_start_time is None:
            request_start_time = time.time()
            
        try:
            start_time = time.time()
            
            # å°†ç»Ÿä¸€æ¨¡å‹åæ˜ å°„åˆ°æä¾›å•†ç‰¹å®šçš„æ¨¡å‹
            actual_model = self.map_model_name(request.model, provider)
            request_data = request.model_dump()
            request_data["model"] = actual_model
            
            logger.info(f"ğŸ“¤ [T+{time.time()-request_start_time:.3f}s] Forwarding request to {provider.name} with model {actual_model} (stream: {request.stream})")
            
            # å¤„ç†æµå¼å“åº”
            if request.stream:
                
                async def stream_generator():
                    """Enhanced stream generator with comprehensive error handling"""
                    client = None
                    connection_id = f"{provider.name}_{int(time.time()*1000)}"
                    
                    try:
                        generator_start = time.time()
                        logger.info(f"ğŸ”„ [T+{generator_start-request_start_time:.3f}s] Stream generator started for {provider.name} (ID: {connection_id})")
                        
                        # åˆ›å»ºä¸“ç”¨å®¢æˆ·ç«¯å®ä¾‹ç”¨äºæµå¤„ç†
                        client = httpx.AsyncClient(
                            timeout=httpx.Timeout(60.0, connect=10.0, read=60.0),
                            limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)
                        )
                        
                        logger.info(f"ğŸ”— [T+{time.time()-request_start_time:.3f}s] HTTP client created for {provider.name}")
                        
                        # ä½¿ç”¨context managerç¡®ä¿è¿æ¥æ­£ç¡®ç®¡ç†
                        async with client.stream(
                            "POST",
                            f"{provider.base_url}/chat/completions",
                            headers={
                                "Authorization": f"Bearer {provider.api_key}",
                                "Content-Type": "application/json",
                                "Connection": "keep-alive",
                                "Accept": "text/event-stream"
                            },
                            json=request_data
                        ) as response:
                            
                            connection_time = time.time() - start_time
                            provider.response_time = (provider.response_time + connection_time) / 2
                            
                            logger.info(f"ğŸŒ [T+{time.time()-request_start_time:.3f}s] Connected to {provider.name} (connection_time: {connection_time:.3f}s, status: {response.status_code})")
                            
                            if response.status_code != 200:
                                error_text = await response.aread()
                                error_message = error_text.decode('utf-8', errors='ignore')
                                logger.error(f"âŒ [T+{time.time()-request_start_time:.3f}s] Provider {provider.name} error {response.status_code}: {error_message}")
                                
                                # è¿”å›æ ‡å‡†é”™è¯¯æ ¼å¼
                                error_response = {
                                    "error": {
                                        "type": "provider_error",
                                        "code": response.status_code,
                                        "message": error_message,
                                        "provider": provider.name
                                    }
                                }
                                yield f'data: {json.dumps(error_response)}\n\n'.encode()
                                return
                            
                            logger.info(f"âœ… [T+{time.time()-request_start_time:.3f}s] Stream response started from {provider.name}")
                            
                            chunk_count = 0
                            last_chunk_time = generator_start
                            total_bytes = 0
                            
                            # ä¼˜åŒ–çš„æµå¼è½¬å‘ï¼šä½¿ç”¨åˆé€‚çš„chunk sizeå¹³è¡¡æ€§èƒ½å’Œå®æ—¶æ€§
                            async for raw_chunk in response.aiter_raw(chunk_size=1024):
                                if raw_chunk:
                                    current_time = time.time()
                                    chunk_count += 1
                                    total_bytes += len(raw_chunk)
                                    
                                    time_since_start = current_time - request_start_time
                                    time_since_last = current_time - last_chunk_time
                                    
                                    # è¯¦ç»†æ—¥å¿—è®°å½•å‰å‡ ä¸ªchunks
                                    if chunk_count <= 3:
                                        logger.info(f"ğŸ“¦ [T+{time_since_start:.3f}s] Chunk {chunk_count}: {len(raw_chunk)} bytes (+{time_since_last*1000:.1f}ms)")
                                    elif chunk_count % 50 == 0:  # æ¯50ä¸ªchunkè®°å½•ä¸€æ¬¡
                                        logger.info(f"ğŸ“Š [T+{time_since_start:.3f}s] Progress: {chunk_count} chunks, {total_bytes} bytes")
                                    
                                    # è½¬å‘chunk
                                    yield raw_chunk
                                    last_chunk_time = current_time
                                    
                                    # æä¾›åå‹æ§åˆ¶
                                    if chunk_count % 100 == 0:
                                        await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
                            
                            completion_time = time.time()
                            total_time = completion_time - generator_start
                            avg_chunk_size = total_bytes / chunk_count if chunk_count > 0 else 0
                            
                            logger.info(f"âœ… [T+{completion_time-request_start_time:.3f}s] Stream completed from {provider.name}: {chunk_count} chunks, {total_bytes} bytes, avg {avg_chunk_size:.1f}B/chunk in {total_time:.3f}s")
                            
                            # æ›´æ–°æä¾›å•†ç»Ÿè®¡
                            provider.error_count = max(0, provider.error_count - 1)  # æˆåŠŸåˆ™å‡å°‘é”™è¯¯è®¡æ•°
                                    
                    except httpx.ConnectError as e:
                        error_time = time.time()
                        logger.error(f"ğŸš« [T+{error_time-request_start_time:.3f}s] Connection error to {provider.name}: {str(e)}")
                        provider.error_count += 1
                        error_response = {
                            "error": {
                                "type": "connection_error",
                                "message": f"Failed to connect to {provider.name}: {str(e)}",
                                "provider": provider.name
                            }
                        }
                        yield f'data: {json.dumps(error_response)}\n\n'.encode()
                        
                    except httpx.TimeoutException as e:
                        error_time = time.time()
                        logger.error(f"â° [T+{error_time-request_start_time:.3f}s] Timeout error from {provider.name}: {str(e)}")
                        provider.error_count += 1
                        error_response = {
                            "error": {
                                "type": "timeout_error", 
                                "message": f"Request to {provider.name} timed out: {str(e)}",
                                "provider": provider.name
                            }
                        }
                        yield f'data: {json.dumps(error_response)}\n\n'.encode()
                        
                    except Exception as e:
                        error_time = time.time()
                        logger.error(f"âŒ [T+{error_time-request_start_time:.3f}s] Unexpected error from {provider.name}: {str(e)}")
                        logger.error(f"âŒ [T+{error_time-request_start_time:.3f}s] Error type: {type(e).__name__}")
                        provider.error_count += 1
                        
                        # è¯¦ç»†é”™è¯¯ä¿¡æ¯
                        import traceback
                        logger.error(f"âŒ [T+{error_time-request_start_time:.3f}s] Traceback: {traceback.format_exc()}")
                        
                        error_response = {
                            "error": {
                                "type": "stream_error",
                                "message": f"Stream error from {provider.name}: {str(e)}",
                                "provider": provider.name
                            }
                        }
                        yield f'data: {json.dumps(error_response)}\n\n'.encode()
                        
                    finally:
                        if client:
                            try:
                                await client.aclose()
                                logger.info(f"ğŸ” [T+{time.time()-request_start_time:.3f}s] HTTP client closed for {provider.name} (ID: {connection_id})")
                            except Exception as close_error:
                                logger.warning(f"âš ï¸ [T+{time.time()-request_start_time:.3f}s] Error closing client for {provider.name}: {close_error}")
                
                logger.info(f"ğŸ—ï¸ [T+{time.time()-request_start_time:.3f}s] Stream generator prepared for {provider.name}")
                return stream_generator()
            
            # å¤„ç†éæµå¼å“åº”
            else:
                logger.info(f"ğŸ“„ [T+{time.time()-request_start_time:.3f}s] Processing non-streaming request to {provider.name}")
                
                try:
                    response = await self.client.post(
                        f"{provider.base_url}/chat/completions",
                        headers={
                            "Authorization": f"Bearer {provider.api_key}",
                            "Content-Type": "application/json"
                        },
                        json=request_data,
                        timeout=60.0
                    )
                    
                    response_time = time.time() - start_time
                    provider.response_time = (provider.response_time + response_time) / 2
                    
                    if response.status_code == 200:
                        result = response.json()
                        
                        # ç»Ÿä¸€åŒ–å“åº”æ ¼å¼
                        result["_router_info"] = {
                            "provider": provider.name,
                            "response_time": response_time,
                            "timestamp": datetime.now().isoformat(),
                            "streaming": False
                        }
                        
                        logger.info(f"âœ… [T+{time.time()-request_start_time:.3f}s] Non-streaming request successful via {provider.name} (response_time: {response_time:.2f}s)")
                        provider.error_count = max(0, provider.error_count - 1)  # æˆåŠŸåˆ™å‡å°‘é”™è¯¯è®¡æ•°
                        return result
                    else:
                        logger.error(f"âŒ [T+{time.time()-request_start_time:.3f}s] Provider {provider.name} returned status {response.status_code}: {response.text}")
                        provider.error_count += 1
                        raise HTTPException(status_code=response.status_code, detail=f"Provider error: {response.text}")
                        
                except httpx.ConnectError as e:
                    logger.error(f"ğŸš« [T+{time.time()-request_start_time:.3f}s] Connection error to {provider.name}: {str(e)}")
                    provider.error_count += 1
                    raise HTTPException(status_code=502, detail=f"Connection error: {str(e)}")
                    
                except httpx.TimeoutException as e:
                    logger.error(f"â° [T+{time.time()-request_start_time:.3f}s] Timeout error from {provider.name}: {str(e)}")
                    provider.error_count += 1
                    raise HTTPException(status_code=504, detail=f"Provider timeout: {str(e)}")
                
        except Exception as e:
            logger.error(f"âŒ [T+{time.time()-request_start_time:.3f}s] Critical error in forward_request: {str(e)}")
            provider.error_count += 1
            raise HTTPException(status_code=502, detail=f"Provider error: {str(e)}")
    
    def map_model_name(self, requested_model: str, provider: AIProvider) -> str:
        """Map generic model names to provider-specific models"""
        
        # é€šç”¨æ¨¡å‹åæ˜ å°„è¡¨
        model_mapping = {
            "gpt-3.5-turbo": {
                "deepseek": "deepseek-chat",
                "lingyiwanwu": "yi-34b-chat", 
                "ollama": "llama3.2"
            },
            "gpt-4": {
                "deepseek": "deepseek-chat",
                "lingyiwanwu": "yi-34b-chat",
                "ollama": "llama3.2"
            },
            "code-assistant": {
                "deepseek": "deepseek-coder",
                "lingyiwanwu": "yi-34b-chat",
                "ollama": "codegemma"
            }
        }
        
        # å¦‚æœæ˜¯é€šç”¨æ¨¡å‹åï¼Œè¿›è¡Œæ˜ å°„
        if requested_model in model_mapping:
            return model_mapping[requested_model].get(provider.name, provider.models[0])
        
        # å¦‚æœæ˜¯æä¾›å•†ç‰¹å®šçš„æ¨¡å‹åï¼Œç›´æ¥ä½¿ç”¨
        if requested_model in provider.models:
            return requested_model
        
        # é»˜è®¤ä½¿ç”¨æä¾›å•†çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
        return provider.models[0]

class LowLevelASGIStreamer:
    """Enhanced low-level ASGI streaming handler for real-time data forwarding"""
    
    def __init__(self, request_start_time: float):
        self.request_start_time = request_start_time
        self.total_bytes_sent = 0
        self.chunks_sent = 0
        self.connection_id = f"asgi_{int(time.time()*1000)}_{id(self)}"
        self.is_headers_sent = False
        self.error_count = 0
        logger.info(f"ğŸ”§ [T+{time.time()-request_start_time:.3f}s] ASGI Streamer initialized (ID: {self.connection_id})")
        
    async def stream_response(self, scope: dict, receive: Callable, send: Callable, 
                            provider: 'AIProvider', request: 'ChatRequest') -> None:
        """Handle streaming response using enhanced low-level ASGI interface"""
        
        client = None
        response = None
        
        try:
            # å‘é€HTTPå“åº”å¤´
            headers_sent_time = time.time()
            logger.info(f"ğŸš€ [T+{headers_sent_time-self.request_start_time:.3f}s] ASGI: Preparing response headers for {provider.name}")
            
            await send({
                'type': 'http.response.start',
                'status': 200,
                'headers': [
                    [b'content-type', b'text/event-stream'],
                    [b'cache-control', b'no-cache'],
                    [b'connection', b'keep-alive'],
                    [b'x-router-provider', provider.name.encode()],
                    [b'x-asgi-streaming', b'true'],
                    [b'x-router-method', b'asgi-parameter-optimized'],
                    [b'x-connection-id', self.connection_id.encode()],
                ],
            })
            self.is_headers_sent = True
            
            headers_completed_time = time.time()
            logger.info(f"âœ… [T+{headers_completed_time-self.request_start_time:.3f}s] ASGI: Headers sent (took {headers_completed_time-headers_sent_time:.3f}s)")
            
            # å»ºç«‹ä¸Šæ¸¸è¿æ¥
            stream_start_time = time.time()
            logger.info(f"ğŸ”„ [T+{stream_start_time-self.request_start_time:.3f}s] ASGI: Starting upstream connection to {provider.name}")
            
            # åˆ›å»ºä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯
            client = httpx.AsyncClient(
                timeout=httpx.Timeout(60.0, connect=10.0, read=60.0),
                limits=httpx.Limits(max_connections=50, max_keepalive_connections=10),
                follow_redirects=False
            )
            
            # å‡†å¤‡è¯·æ±‚æ•°æ®
            actual_model = self.map_model_name(request.model, provider)
            request_data = request.model_dump()
            request_data["model"] = actual_model
            
            upstream_start_time = time.time()
            logger.info(f"ğŸ“¡ [T+{upstream_start_time-self.request_start_time:.3f}s] ASGI: Connecting to {provider.name} with model {actual_model}")
            
            # ä½¿ç”¨stream context manager
            async with client.stream(
                "POST",
                f"{provider.base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {provider.api_key}",
                    "Content-Type": "application/json",
                    "Accept": "text/event-stream",
                    "Connection": "keep-alive"
                },
                json=request_data
            ) as response:
                
                upstream_connected_time = time.time()
                connection_latency = upstream_connected_time - upstream_start_time
                logger.info(f"ğŸ”— [T+{upstream_connected_time-self.request_start_time:.3f}s] ASGI: Connected to upstream (latency: {connection_latency:.3f}s, status: {response.status_code})")
                
                if response.status_code != 200:
                    error_text = await response.aread()
                    error_message = error_text.decode('utf-8', errors='ignore')
                    logger.error(f"âŒ [T+{time.time()-self.request_start_time:.3f}s] ASGI: Upstream error {response.status_code}: {error_message}")
                    
                    error_data = {
                        "error": {
                            "type": "upstream_error",
                            "code": response.status_code,
                            "message": error_message,
                            "provider": provider.name,
                            "connection_id": self.connection_id
                        }
                    }
                    error_json = f'data: {json.dumps(error_data)}\n\n'.encode()
                    
                    await send({
                        'type': 'http.response.body',
                        'body': error_json,
                        'more_body': False,
                    })
                    return
                
                # å¼€å§‹å®æ—¶è½¬å‘æ•°æ®
                logger.info(f"ğŸš€ [T+{time.time()-self.request_start_time:.3f}s] ASGI: Starting optimized real-time data forwarding")
                
                last_chunk_time = time.time()
                performance_samples = []
                
                # ä½¿ç”¨ä¼˜åŒ–çš„chunk sizeä»¥å¹³è¡¡æ€§èƒ½å’Œå®æ—¶æ€§
                chunk_size = 1024  # 1KB chunks for good balance
                async for chunk in response.aiter_bytes(chunk_size=chunk_size):
                    if chunk:
                        current_time = time.time()
                        self.chunks_sent += 1
                        self.total_bytes_sent += len(chunk)
                        
                        time_since_start = current_time - self.request_start_time
                        time_since_last = current_time - last_chunk_time
                        performance_samples.append(time_since_last)
                        
                        # è¯¦ç»†è®°å½•å‰å‡ ä¸ªchunks
                        if self.chunks_sent <= 3:
                            logger.info(f"ğŸ“¦ [T+{time_since_start:.3f}s] ASGI: Chunk {self.chunks_sent}: {len(chunk)} bytes (+{time_since_last*1000:.1f}ms)")
                        elif self.chunks_sent % 100 == 0:  # æ¯100ä¸ªchunkè®°å½•ä¸€æ¬¡æ€§èƒ½
                            avg_interval = sum(performance_samples[-100:]) / min(100, len(performance_samples))
                            logger.info(f"ğŸ“Š [T+{time_since_start:.3f}s] ASGI: Progress {self.chunks_sent} chunks, {self.total_bytes_sent} bytes (avg: {avg_interval*1000:.1f}ms)")
                        
                        # ç«‹å³é€šè¿‡ASGIå‘é€æ•°æ®
                        send_start_time = time.time()
                        await send({
                            'type': 'http.response.body',
                            'body': chunk,
                            'more_body': True,
                        })
                        send_duration = time.time() - send_start_time
                        
                        # è®°å½•å‘é€æ€§èƒ½
                        if self.chunks_sent <= 3:
                            logger.info(f"ğŸ“¤ [T+{time.time()-self.request_start_time:.3f}s] ASGI: Chunk {self.chunks_sent} sent (send: {send_duration*1000:.1f}ms)")
                        
                        last_chunk_time = current_time
                        
                        # æ§åˆ¶æµé‡ä»¥é˜²æ­¢è¿‡è½½
                        if self.chunks_sent % 50 == 0:
                            await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
                
                # å‘é€ç»“æŸä¿¡å·
                completion_time = time.time()
                total_duration = completion_time - stream_start_time
                avg_chunk_size = self.total_bytes_sent / self.chunks_sent if self.chunks_sent > 0 else 0
                
                # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
                if performance_samples:
                    avg_interval = sum(performance_samples) / len(performance_samples)
                    min_interval = min(performance_samples)
                    max_interval = max(performance_samples)
                    zero_intervals = sum(1 for x in performance_samples if x < 0.001)  # <1ms
                    zero_percentage = (zero_intervals / len(performance_samples)) * 100
                else:
                    avg_interval = min_interval = max_interval = 0
                    zero_percentage = 0
                
                logger.info(f"ğŸ [T+{completion_time-self.request_start_time:.3f}s] ASGI: Stream completed")
                logger.info(f"ğŸ“Š [T+{completion_time-self.request_start_time:.3f}s] ASGI: Performance - {self.chunks_sent} chunks, {self.total_bytes_sent} bytes, avg {avg_chunk_size:.1f}B/chunk")
                logger.info(f"â±ï¸ [T+{completion_time-self.request_start_time:.3f}s] ASGI: Timing - total {total_duration:.3f}s, avg interval {avg_interval*1000:.1f}ms, zero intervals {zero_percentage:.1f}%")
                
                await send({
                    'type': 'http.response.body',
                    'body': b'',
                    'more_body': False,
                })
                
                logger.info(f"âœ… [T+{completion_time-self.request_start_time:.3f}s] ASGI: Response completed successfully")
                
                # æ›´æ–°providerç»Ÿè®¡
                provider.error_count = max(0, provider.error_count - 1)  # æˆåŠŸå‡å°‘é”™è¯¯è®¡æ•°
                    
        except httpx.ConnectError as e:
            error_time = time.time()
            self.error_count += 1
            logger.error(f"ğŸš« [T+{error_time-self.request_start_time:.3f}s] ASGI: Connection error: {str(e)}")
            await self._send_error_response(send, "connection_error", f"Connection failed: {str(e)}", provider.name)
            
        except httpx.TimeoutException as e:
            error_time = time.time()
            self.error_count += 1
            logger.error(f"â° [T+{error_time-self.request_start_time:.3f}s] ASGI: Timeout error: {str(e)}")
            await self._send_error_response(send, "timeout_error", f"Request timeout: {str(e)}", provider.name)
            
        except Exception as e:
            error_time = time.time()
            self.error_count += 1
            logger.error(f"âŒ [T+{error_time-self.request_start_time:.3f}s] ASGI: Unexpected error: {str(e)}")
            logger.error(f"âŒ [T+{error_time-self.request_start_time:.3f}s] ASGI: Error type: {type(e).__name__}")
            
            # è¯¦ç»†é”™è¯¯ä¿¡æ¯
            import traceback
            logger.error(f"âŒ [T+{error_time-self.request_start_time:.3f}s] ASGI: Traceback: {traceback.format_exc()}")
            
            await self._send_error_response(send, "stream_error", f"Stream error: {str(e)}", provider.name)
            
        finally:
            # æ¸…ç†èµ„æº
            if client:
                try:
                    await client.aclose()
                    logger.info(f"ğŸ” [T+{time.time()-self.request_start_time:.3f}s] ASGI: HTTP client closed (ID: {self.connection_id})")
                except Exception as close_error:
                    logger.warning(f"âš ï¸ [T+{time.time()-self.request_start_time:.3f}s] ASGI: Error closing client: {close_error}")
    
    async def _send_error_response(self, send: Callable, error_type: str, message: str, provider_name: str):
        """Send error response through ASGI"""
        try:
            if not self.is_headers_sent:
                await send({
                    'type': 'http.response.start',
                    'status': 500,
                    'headers': [
                        [b'content-type', b'text/event-stream'],
                        [b'x-router-error', error_type.encode()],
                        [b'x-connection-id', self.connection_id.encode()],
                    ],
                })
                self.is_headers_sent = True
            
            error_data = {
                "error": {
                    "type": error_type,
                    "message": message,
                    "provider": provider_name,
                    "connection_id": self.connection_id
                }
            }
            error_json = f'data: {json.dumps(error_data)}\n\n'.encode()
            
            await send({
                'type': 'http.response.body',
                'body': error_json,
                'more_body': False,
            })
        except Exception as send_error:
            logger.error(f"âŒ [T+{time.time()-self.request_start_time:.3f}s] ASGI: Failed to send error response: {send_error}")
    
    def map_model_name(self, requested_model: str, provider: 'AIProvider') -> str:
        """Map generic model names to provider-specific models"""
        
        # é€šç”¨æ¨¡å‹åæ˜ å°„è¡¨
        model_mapping = {
            "gpt-3.5-turbo": {
                "deepseek": "deepseek-chat",
                "lingyiwanwu": "yi-34b-chat", 
                "ollama": "llama3.2"
            },
            "gpt-4": {
                "deepseek": "deepseek-chat",
                "lingyiwanwu": "yi-34b-chat",
                "ollama": "llama3.2"
            },
            "code-assistant": {
                "deepseek": "deepseek-coder",
                "lingyiwanwu": "yi-34b-chat",
                "ollama": "codegemma"
            }
        }
        
        # å¦‚æœæ˜¯é€šç”¨æ¨¡å‹åï¼Œè¿›è¡Œæ˜ å°„
        if requested_model in model_mapping:
            return model_mapping[requested_model].get(provider.name, provider.models[0])
        
        # å¦‚æœæ˜¯æä¾›å•†ç‰¹å®šçš„æ¨¡å‹åï¼Œç›´æ¥ä½¿ç”¨
        if requested_model in provider.models:
            return requested_model
        
        # é»˜è®¤ä½¿ç”¨æä¾›å•†çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
        return provider.models[0]

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="Smart AI Router", version="1.1.0")
router = SmartAIRouter()

# èº«ä»½éªŒè¯è®¾ç½®
security = HTTPBearer()

async def authenticate(credentials: HTTPAuthorizationCredentials = Depends(security)) -> UserInfo:
    """APIå¯†é’¥èº«ä»½éªŒè¯ä¾èµ–å‡½æ•°"""
    api_key = credentials.credentials
    
    user_info = router.verify_api_key(api_key)
    if not user_info:
        logger.warning(f"ğŸš« Invalid API key attempted: {api_key[:10]}...")
        raise HTTPException(
            status_code=401, 
            detail="Invalid API key"
        )
    
    if not user_info.is_active:
        logger.warning(f"ğŸš« Inactive user attempted access: {user_info.user_id}")
        raise HTTPException(
            status_code=403, 
            detail="User account is inactive"
        )
    
    if not router.check_quota(user_info):
        logger.warning(f"ğŸ“Š Quota exceeded for user: {user_info.user_id}")
        raise HTTPException(
            status_code=429, 
            detail=f"Daily quota exceeded. Limit: {user_info.daily_limit}, Used: {user_info.requests_today}"
        )
    
    logger.info(f"ğŸ”“ Authenticated user: {user_info.user_id} (requests today: {user_info.requests_today}/{user_info.daily_limit})")
    return user_info

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„äº‹ä»¶å¤„ç†"""
    logger.info("ğŸš€ Starting Smart AI Router application...")
    
    # æ˜¾ç¤ºæ‰€æœ‰æ³¨å†Œçš„è·¯ç”±
    logger.info("ğŸ“‹ Registered routes:")
    for route in app.routes:
        if hasattr(route, 'path') and hasattr(route, 'methods'):
            methods = getattr(route, 'methods', set())
            logger.info(f"  ğŸ“ {route.path} - {methods}")
    
    await router.start_health_checks()

@app.post("/v1/chat/completions")
async def chat_completions(request: Request, user_info: UserInfo = Depends(authenticate)):
    """Unified chat completions endpoint with authentication and streaming support"""
    start_time = time.time()  # è¯·æ±‚å¼€å§‹æ—¶é—´åŸºå‡†
    logger.info(f"ğŸš€ [T+0.000s] === CHAT COMPLETIONS REQUEST START === for user: {user_info.user_id}")
    
    try:
        router.stats["total_requests"] += 1
        
        # æ›´æ–°ç”¨æˆ·ç»Ÿè®¡
        router.update_user_stats(user_info)
        
        # è¯»å–åŸå§‹è¯·æ±‚ä½“
        request_read_start = time.time()
        request_data = await request.json()
        request_read_time = time.time() - request_read_start
        logger.info(f"ğŸ“¥ [T+{time.time()-start_time:.3f}s] Raw request data: {json.dumps(request_data, indent=2)}")
        logger.info(f"ğŸ“¥ [T+{time.time()-start_time:.3f}s] Request headers: {dict(request.headers)}")
        logger.info(f"ğŸ“¥ [T+{time.time()-start_time:.3f}s] Request parsed: stream={request_data.get('stream', 'NOT_SET')} (parse took {request_read_time:.3f}s)")
        
        # å¦‚æœå®¢æˆ·ç«¯æ²¡æœ‰æŒ‡å®šstreamå‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨æµå¼è¾“å‡º
        if "stream" not in request_data:
            request_data["stream"] = True
            logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] Setting default stream=True")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä½å±‚ASGIæµå¤„ç†ï¼ˆæ¨èæ–¹æ³•ï¼‰
        asgi_from_data = request_data.get("x_use_asgi_streaming", False)
        asgi_from_header = request.headers.get("x-use-asgi-streaming") == "true"
        use_asgi_streaming = asgi_from_data or asgi_from_header
        
        logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] === STREAMING METHOD DETECTION ===")
        logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] x_use_asgi_streaming from request_data: {asgi_from_data}")
        logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] x-use-asgi-streaming from header: {request.headers.get('x-use-asgi-streaming', 'NOT_SET')} -> {asgi_from_header}")
        logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] Final use_asgi_streaming decision: {use_asgi_streaming}")
        
        if use_asgi_streaming:
            logger.info(f"âœ… [T+{time.time()-start_time:.3f}s] *** USING RECOMMENDED ASGI STREAMING METHOD ***")
        else:
            logger.info(f"ğŸ“¦ [T+{time.time()-start_time:.3f}s] Using FastAPI streaming (fallback)")
        
        # åˆ›å»ºChatRequestå¯¹è±¡
        chat_request_start = time.time()
        chat_request = ChatRequest(**request_data)
        chat_request_time = time.time() - chat_request_start
        logger.info(f"ğŸ“¤ [T+{time.time()-start_time:.3f}s] ChatRequest created: stream={chat_request.stream}, asgi_streaming={use_asgi_streaming} (took {chat_request_time:.3f}s)")
        
        # é€‰æ‹©æä¾›å•†
        provider_select_start = time.time()
        provider = router.select_provider(chat_request)
        provider_select_time = time.time() - provider_select_start
        
        if not provider:
            logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] No available providers!")
            router.stats["failed_requests"] += 1
            raise HTTPException(status_code=503, detail="No available AI providers")
        
        logger.info(f"ğŸ¯ [T+{time.time()-start_time:.3f}s] Provider selected: {provider.name} (selection took {provider_select_time:.3f}s)")
        
        # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
        router.stats["provider_usage"][provider.name] += 1
        
        # å¤„ç†æµå¼å“åº”
        if chat_request.stream:
            logger.info(f"ğŸŒŠ [T+{time.time()-start_time:.3f}s] === ENTERING STREAMING MODE ===")
            
            if use_asgi_streaming:
                # ä½¿ç”¨æ¨èçš„ASGIæµå¤„ç†æ–¹æ³•
                logger.info(f"ğŸš€ [T+{time.time()-start_time:.3f}s] *** INITIALIZING RECOMMENDED ASGI STREAMING ***")
                
                # éªŒè¯providerå¥åº·çŠ¶æ€
                if provider.error_count > 3:
                    logger.warning(f"âš ï¸ [T+{time.time()-start_time:.3f}s] Provider {provider.name} has high error count: {provider.error_count}")
                
                # åˆ›å»ºå¢å¼ºçš„ASGIæµå¤„ç†å™¨
                asgi_streamer = LowLevelASGIStreamer(start_time)
                logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] Enhanced ASGI streamer created")
                
                # è·å–ASGIä½œç”¨åŸŸä¿¡æ¯
                scope = request.scope
                logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] ASGI scope obtained: {scope.get('type', 'unknown')}")
                
                # åˆ›å»ºä¼˜åŒ–çš„ASGIå“åº”
                class EnhancedASGIStreamingResponse(Response):
                    def __init__(self, streamer: LowLevelASGIStreamer, provider: AIProvider, chat_request: ChatRequest):
                        self.streamer = streamer
                        self.provider = provider
                        self.chat_request = chat_request
                        self.request_start_time = start_time
                        logger.info(f"ğŸ”§ [T+{time.time()-start_time:.3f}s] Enhanced ASGI Response created for {provider.name}")
                        super().__init__()
                    
                    async def __call__(self, scope, receive, send):
                        try:
                            logger.info(f"ğŸ”§ [T+{time.time()-self.request_start_time:.3f}s] Enhanced ASGI Response.__call__ started")
                            await self.streamer.stream_response(scope, receive, send, self.provider, self.chat_request)
                            logger.info(f"ğŸ”§ [T+{time.time()-self.request_start_time:.3f}s] Enhanced ASGI Response.__call__ completed successfully")
                        except Exception as e:
                            logger.error(f"âŒ [T+{time.time()-self.request_start_time:.3f}s] ASGI Response error: {str(e)}")
                            # å‘é€é”™è¯¯å“åº”
                            try:
                                error_data = f'data: {{"error": {{"type": "asgi_error", "message": "{str(e)}", "provider": "{self.provider.name}"}}}}\n\n'.encode()
                                await send({
                                    'type': 'http.response.start',
                                    'status': 500,
                                    'headers': [[b'content-type', b'text/event-stream']],
                                })
                                await send({
                                    'type': 'http.response.body',
                                    'body': error_data,
                                    'more_body': False,
                                })
                            except:
                                pass  # å¦‚æœæ— æ³•å‘é€é”™è¯¯ï¼Œåˆ™é™é»˜å¤±è´¥
                
                router.stats["successful_requests"] += 1
                logger.info(f"âœ… [T+{time.time()-start_time:.3f}s] *** RETURNING ENHANCED ASGI STREAMING RESPONSE ***")
                return EnhancedASGIStreamingResponse(asgi_streamer, provider, chat_request)
            
            else:
                # ä½¿ç”¨æ”¹è¿›çš„FastAPIæµå¤„ç†ï¼ˆä½œä¸ºfallbackï¼‰
                logger.info(f"ğŸ“¦ [T+{time.time()-start_time:.3f}s] *** USING IMPROVED FASTAPI STREAMING ***")
                forward_start = time.time()
                logger.info(f"ğŸ“¡ [T+{time.time()-start_time:.3f}s] Starting FastAPI streaming request forwarding to {provider.name}")
                
                try:
                    result = await router.forward_request(provider, chat_request, start_time)
                    forward_setup_time = time.time() - forward_start
                    logger.info(f"ğŸ”— [T+{time.time()-start_time:.3f}s] FastAPI forward setup completed (took {forward_setup_time:.3f}s)")
                    
                    stream_response_start = time.time()
                    logger.info(f"âœ… [T+{stream_response_start-start_time:.3f}s] Creating FastAPI StreamingResponse")
                    router.stats["successful_requests"] += 1
                    
                    # åŒ…è£…ç”Ÿæˆå™¨ä»¥æ·»åŠ é”™è¯¯å¤„ç†
                    async def error_handled_stream():
                        stream_wrapper_start = time.time()
                        logger.info(f"ğŸ”„ [T+{stream_wrapper_start-start_time:.3f}s] FastAPI stream wrapper started")
                        
                        chunk_count = 0
                        try:
                            async for chunk in result:
                                chunk_count += 1
                                chunk_time = time.time()
                                if chunk_count <= 5:  # Log first 5 chunks in detail
                                    logger.info(f"ğŸ“¡ [T+{chunk_time-start_time:.3f}s] FastAPI wrapper forwarding chunk {chunk_count}: {len(chunk) if chunk else 0} bytes")
                                yield chunk
                        except Exception as e:
                            logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] FastAPI stream wrapper error: {str(e)}")
                            error_response = {
                                "error": {
                                    "type": "fastapi_stream_error",
                                    "message": str(e),
                                    "provider": provider.name
                                }
                            }
                            yield f'data: {json.dumps(error_response)}\n\n'.encode()
                        finally:
                            stream_wrapper_end = time.time()
                            logger.info(f"ğŸ [T+{stream_wrapper_end-start_time:.3f}s] FastAPI stream wrapper completed: {chunk_count} chunks")
                    
                    response_creation_time = time.time()
                    logger.info(f"ğŸ¯ [T+{response_creation_time-start_time:.3f}s] Returning FastAPI StreamingResponse to client")
                    
                    return StreamingResponse(
                        error_handled_stream(),
                        media_type="text/event-stream",
                        headers={
                            "Cache-Control": "no-cache",
                            "Connection": "keep-alive",
                            "X-Router-Provider": provider.name,
                            "X-Router-User": user_info.user_id,
                            "X-Streaming-Type": "fastapi-improved",
                            "X-Router-Recommendation": "use-asgi-streaming"
                        }
                    )
                    
                except Exception as e:
                    logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] FastAPI streaming setup failed: {str(e)}")
                    router.stats["failed_requests"] += 1
                    raise HTTPException(status_code=500, detail=f"Streaming setup failed: {str(e)}")
        
        # å¤„ç†éæµå¼å“åº”
        else:
            logger.info(f"ğŸ“„ [T+{time.time()-start_time:.3f}s] === ENTERING NON-STREAMING MODE ===")
            forward_start = time.time()
            logger.info(f"ğŸ“¡ [T+{time.time()-start_time:.3f}s] Starting non-streaming request forwarding to {provider.name}")
            
            try:
                result = await router.forward_request(provider, chat_request, start_time)
                forward_setup_time = time.time() - forward_start
                logger.info(f"ğŸ”— [T+{time.time()-start_time:.3f}s] Non-streaming forward completed (took {forward_setup_time:.3f}s)")
                
                logger.info(f"ğŸ“„ [T+{time.time()-start_time:.3f}s] Returning JSON response")
                # æ·»åŠ ç”¨æˆ·ä¿¡æ¯åˆ°å“åº”
                if "_router_info" not in result:
                    result["_router_info"] = {}
                result["_router_info"]["user_id"] = user_info.user_id
                result["_router_info"]["requests_today"] = user_info.requests_today
                result["_router_info"]["daily_limit"] = user_info.daily_limit
                result["_router_info"]["recommendation"] = "Use x_use_asgi_streaming: true for optimal streaming"
                
                router.stats["successful_requests"] += 1
                return result
                
            except Exception as e:
                logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] Non-streaming request failed: {str(e)}")
                router.stats["failed_requests"] += 1
                raise
        
    except HTTPException as he:
        logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] HTTP Exception: {he.status_code} - {he.detail}")
        router.stats["failed_requests"] += 1
        raise
    except Exception as e:
        router.stats["failed_requests"] += 1
        logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] Unexpected error: {str(e)}")
        logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] Error type: {type(e).__name__}")
        import traceback
        logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail="Internal server error")
    finally:
        end_time = time.time()
        total_time = end_time - start_time
        logger.info(f"ğŸ [T+{total_time:.3f}s] === CHAT COMPLETIONS REQUEST END === (total time: {total_time:.3f}s)")

@app.get("/health")
async def health_check():
    """Health check endpoint (no authentication required)"""
    healthy_providers = [p.name for p in router.providers if p.status == ProviderStatus.HEALTHY]
    
    return {
        "status": "healthy" if healthy_providers else "unhealthy",
        "providers": {
            provider.name: {
                "status": provider.status.value,
                "response_time": provider.response_time,
                "error_count": provider.error_count,
                "last_check": provider.last_check
            }
            for provider in router.providers
        },
        "stats": router.stats,
        "auth_mode": router.auth_mode
    }

@app.get("/stats")
async def get_stats(user_info: UserInfo = Depends(authenticate)):
    """Get usage statistics (requires authentication)"""
    return {
        "stats": router.stats,
        "providers": [
            {
                "name": p.name,
                "status": p.status.value,
                "weight": p.weight,
                "response_time": p.response_time,
                "error_count": p.error_count,
                "models": p.models
            }
            for p in router.providers
        ],
        "user_info": {
            "user_id": user_info.user_id,
            "requests_today": user_info.requests_today,
            "daily_limit": user_info.daily_limit,
            "total_requests": user_info.total_requests
        }
    }

@app.get("/auth/info")
async def get_auth_info():
    """Get authentication configuration info (no auth required)"""
    return {
        "auth_mode": router.auth_mode,
        "shared_key_format": "sk-router-2024-unified-api-key" if router.auth_mode == "shared" else None,
        "multi_user_keys": [
            {
                "key_prefix": key[:15] + "...",
                "user_id": user.user_id,
                "daily_limit": user.daily_limit
            }
            for key, user in router.users_db.items()
        ] if router.auth_mode == "multi_user" else None,
        "usage_note": "Include 'Authorization: Bearer YOUR_API_KEY' header in requests"
    }

@app.get("/v1/models")
async def list_models():
    """List available models endpoint - shows unified DeepSeek models only"""
    # ç»Ÿä¸€å¯¹å¤–å±•ç¤ºçš„ DeepSeek æ¨¡å‹åˆ—è¡¨
    unified_models = [
        {
            "id": "deepseek-chat",
            "object": "model",
            "created": 1677649963,
            "owned_by": "deepseek",
            "permission": [
                {
                    "id": "modelperm-deepseek-chat",
                    "object": "model_permission", 
                    "created": 1677649963,
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }
            ],
            "root": "deepseek-chat",
            "parent": None
        },
        {
            "id": "deepseek-coder",
            "object": "model",
            "created": 1677649963,
            "owned_by": "deepseek",
            "permission": [
                {
                    "id": "modelperm-deepseek-coder",
                    "object": "model_permission",
                    "created": 1677649963,
                    "allow_create_engine": False,
                    "allow_sampling": True,
                    "allow_logprobs": True,
                    "allow_search_indices": False,
                    "allow_view": True,
                    "allow_fine_tuning": False,
                    "organization": "*",
                    "group": None,
                    "is_blocking": False
                }
            ],
            "root": "deepseek-coder",
            "parent": None
        }
    ]
    
    return {
        "object": "list",
        "data": unified_models
    }

@app.post("/v1/chat/completions/asgi")
async def chat_completions_asgi(request: Request, user_info: UserInfo = Depends(authenticate)):
    """Specialized endpoint for ASGI streaming (always uses low-level streaming)"""
    start_time = time.time()
    logger.info(f"ğŸš€ [T+0.000s] ASGI endpoint request started for user: {user_info.user_id}")
    
    try:
        router.stats["total_requests"] += 1
        router.update_user_stats(user_info)
        
        # è¯»å–è¯·æ±‚
        request_data = await request.json()
        request_data["stream"] = True  # å¼ºåˆ¶å¯ç”¨æµå¼è¾“å‡º
        request_data["x_use_asgi_streaming"] = True  # å¼ºåˆ¶å¯ç”¨ASGIæµå¤„ç†
        
        chat_request = ChatRequest(**request_data)
        logger.info(f"ğŸ“¤ [T+{time.time()-start_time:.3f}s] ASGI endpoint: stream={chat_request.stream}")
        
        # é€‰æ‹©æä¾›å•†
        provider = router.select_provider(chat_request)
        if not provider:
            raise HTTPException(status_code=503, detail="No available AI providers")
        
        logger.info(f"ğŸ¯ [T+{time.time()-start_time:.3f}s] ASGI endpoint: selected {provider.name}")
        router.stats["provider_usage"][provider.name] += 1
        
        # åˆ›å»ºASGIæµå¤„ç†å™¨
        asgi_streamer = LowLevelASGIStreamer(start_time)
        
        # è‡ªå®šä¹‰ASGIå“åº”
        class ASGIStreamingResponse(Response):
            def __init__(self, streamer: LowLevelASGIStreamer, provider: AIProvider, chat_request: ChatRequest):
                self.streamer = streamer
                self.provider = provider
                self.chat_request = chat_request
                super().__init__()
            
            async def __call__(self, scope, receive, send):
                await self.streamer.stream_response(scope, receive, send, self.provider, self.chat_request)
        
        router.stats["successful_requests"] += 1
        return ASGIStreamingResponse(asgi_streamer, provider, chat_request)
        
    except Exception as e:
        router.stats["failed_requests"] += 1
        logger.error(f"âŒ [T+{time.time()-start_time:.3f}s] ASGI endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ASGI streaming error: {str(e)}")

@app.get("/streaming-compare")
async def streaming_compare():
    """Endpoint to compare different streaming methods"""
    return {
        "streaming_methods": {
            "fastapi": {
                "endpoint": "/v1/chat/completions",
                "method": "POST",
                "headers": {"X-Streaming-Type": "fastapi"},
                "description": "Uses FastAPI's built-in StreamingResponse"
            },
            "asgi": {
                "endpoint": "/v1/chat/completions/asgi", 
                "method": "POST",
                "headers": {"X-Streaming-Type": "asgi"},
                "description": "Uses low-level ASGI interface for real-time streaming"
            },
            "hybrid": {
                "endpoint": "/v1/chat/completions",
                "method": "POST", 
                "body_param": {"x_use_asgi_streaming": True},
                "headers": {"x-use-asgi-streaming": "true"},
                "description": "Uses ASGI streaming via parameter or header"
            }
        },
        "test_commands": {
            "fastapi_test": 'curl -X POST "http://localhost:9000/v1/chat/completions" -H "Content-Type: application/json" -H "Authorization: Bearer sk-8d6804b011614dba7bd065f8644514b" -d \'{"model": "deepseek-chat", "messages": [{"role": "user", "content": "hello"}], "max_tokens": 8}\'',
            "asgi_test": 'curl -X POST "http://localhost:9000/v1/chat/completions/asgi" -H "Content-Type: application/json" -H "Authorization: Bearer sk-8d6804b011614dba7bd065f8644514b" -d \'{"model": "deepseek-chat", "messages": [{"role": "user", "content": "hello"}], "max_tokens": 8}\'',
            "hybrid_test": 'curl -X POST "http://localhost:9000/v1/chat/completions" -H "Content-Type: application/json" -H "Authorization: Bearer sk-8d6804b011614dba7bd065f8644514b" -H "x-use-asgi-streaming: true" -d \'{"model": "deepseek-chat", "messages": [{"role": "user", "content": "hello"}], "max_tokens": 8}\''
        }
    }

if __name__ == "__main__":
    logger.info("ğŸš€ Starting Smart AI Router...")
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=9000,
        log_level="info"
    ) 